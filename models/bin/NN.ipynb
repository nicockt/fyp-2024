{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9c8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util_visualization import plot_feature_importances, plot_roc_curve\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_percentage_error, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from pylab import rcParams\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pickle\n",
    "import importlib\n",
    "import sys\n",
    "import joblib\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import glob\n",
    "\n",
    "#For NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.setrecursionlimit(10**6)  # Set the recursion limit to a higher value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73ec8a",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72dfd19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            volatility      index  crash_label  price_change  volume_change  \\\n",
      "date                                                                          \n",
      "1998-03-31   -0.628541  000001.SS            0      0.187709      -0.127147   \n",
      "1998-06-30   -0.628541  000001.SS            0      0.504913      -0.127147   \n",
      "1998-09-30   -0.625680  000001.SS            1     -0.795446      -0.127147   \n",
      "1998-12-31   -0.631403  000001.SS            0     -0.843210      -0.127147   \n",
      "1999-03-31   -0.629515  000001.SS            0     -0.082838      -0.127147   \n",
      "...                ...        ...          ...           ...            ...   \n",
      "2018-12-31   -0.493234      ^SSMI            0     -0.800378      -0.127147   \n",
      "2019-03-31   -0.529809      ^SSMI            0      0.914383      -0.127147   \n",
      "2019-06-30   -0.458321      ^SSMI            0      0.217264      -0.127147   \n",
      "2019-09-30   -0.388212      ^SSMI            0     -0.010617      -0.127147   \n",
      "2019-12-31   -0.351813      ^SSMI            0      0.296414      -0.127147   \n",
      "\n",
      "                  date  Quarter  Crude_Oil_Index_Excess_Return  Ted_Rate  \\\n",
      "date                                                                       \n",
      "1998-03-31  1998-03-31  Q1 1998                      -0.972362 -0.728639   \n",
      "1998-06-30  1998-06-30  Q2 1998                      -1.203787  0.454432   \n",
      "1998-09-30  1998-09-30  Q3 1998                       0.300444  0.232910   \n",
      "1998-12-31  1998-12-31  Q4 1998                      -1.780450  0.792262   \n",
      "1999-03-31  1999-03-31  Q1 1999                       2.007257 -1.475722   \n",
      "...                ...      ...                            ...       ...   \n",
      "2018-12-31  2018-12-31  Q4 2018                      -2.237575  0.008073   \n",
      "2019-03-31  2019-03-31  Q1 2019                       1.627393 -0.286139   \n",
      "2019-06-30  2019-06-30  Q2 2019                      -0.261086 -1.140095   \n",
      "2019-09-30  2019-09-30  Q3 2019                      -0.487322  0.017963   \n",
      "2019-12-31  2019-12-31  Q4 2019                       0.671579  2.006840   \n",
      "\n",
      "            Gold_Price  ...  10YGov_BondYield  current_acct   FX_Rate  \\\n",
      "date                    ...                                             \n",
      "1998-03-31   -0.382511  ...         -0.007242     -0.029370 -0.079493   \n",
      "1998-06-30   -0.514805  ...         -0.066319      0.036556 -0.079493   \n",
      "1998-09-30   -1.384953  ...         -0.321596      0.037809 -0.079493   \n",
      "1998-12-31    2.097070  ...          0.231035      0.033007 -0.079493   \n",
      "1999-03-31   -1.057305  ...          0.028723      0.019243 -0.079493   \n",
      "...                ...  ...               ...           ...       ...   \n",
      "2018-12-31    0.457541  ...         -6.976072      0.025428 -0.079493   \n",
      "2019-03-31    0.294116  ...          0.507654      0.008938  0.400113   \n",
      "2019-06-30    0.285052  ...          2.466357      0.052412 -0.549603   \n",
      "2019-09-30    1.413067  ...         -0.367367     -0.086759 -0.079493   \n",
      "2019-12-31   -0.772406  ...          0.659285      0.040419 -0.559100   \n",
      "\n",
      "            turnover  Population       npl  Recession_Indicators  inflation  \\\n",
      "date                                                                          \n",
      "1998-03-31  0.658318    0.574981  0.203637              1.121317  -0.029370   \n",
      "1998-06-30  0.658318    0.574981  0.203637              1.121317   0.036556   \n",
      "1998-09-30  0.658318    0.574981  0.203637              1.121317   0.037809   \n",
      "1998-12-31  0.658318    0.574981  0.203637              1.121317   0.033007   \n",
      "1999-03-31 -0.324399    0.391240  0.201522              1.121317   0.019243   \n",
      "...              ...         ...       ...                   ...        ...   \n",
      "2018-12-31  0.033040    0.125618 -0.349336              1.121317   0.025428   \n",
      "2019-03-31 -0.311569    0.084730 -0.351040              1.121317   0.008938   \n",
      "2019-06-30 -0.311569    0.084730  2.680854              1.121317   0.052412   \n",
      "2019-09-30 -0.311569    0.084730  6.265181              1.121317  -0.086759   \n",
      "2019-12-31 -0.311569    0.084730  9.849549              1.121317   0.040419   \n",
      "\n",
      "            Unemployment       GDP  \n",
      "date                                \n",
      "1998-03-31      0.066845  0.246733  \n",
      "1998-06-30      0.066845  0.246733  \n",
      "1998-09-30      0.066845  0.246733  \n",
      "1998-12-31      0.066845  0.246733  \n",
      "1999-03-31      0.311972  0.173266  \n",
      "...                  ...       ...  \n",
      "2018-12-31     -0.082835 -0.032256  \n",
      "2019-03-31     -0.487134 -0.569956  \n",
      "2019-06-30     -0.487134 -0.569956  \n",
      "2019-09-30     -0.487134 -0.569956  \n",
      "2019-12-31     -0.487134 -0.569956  \n",
      "\n",
      "[880 rows x 22 columns]\n",
      "['volatility', 'price_change', 'volume_change', 'Crude_Oil_Index_Excess_Return', 'Ted_Rate', 'Gold_Price', 'housing', 'reserve', '10YGov_BondYield', 'current_acct', 'FX_Rate', 'turnover', 'Population', 'npl', 'Recession_Indicators', 'inflation', 'Unemployment', 'GDP']\n"
     ]
    }
   ],
   "source": [
    "#Import Dataset\n",
    "labeled_index = pd.read_csv(\"../data/labeled_data/quarterly_labeled_features_standardized.csv\")\n",
    "labeled_index.set_index(labeled_index['date'], inplace=True)\n",
    "quarters = np.sort(labeled_index.index.unique())\n",
    "print(labeled_index)\n",
    "\n",
    "#Features Columns\n",
    "features_columns = list(labeled_index.columns)\n",
    "columns_to_remove = ['index', 'crash_label', 'date', 'Quarter']\n",
    "features_columns = [column for column in features_columns if column not in columns_to_remove]\n",
    "print(features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb2c578",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e566af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Custom estimator class for grid search compatibility\n",
    "class NeuralNetworkEstimator(nn.Module, GridSearchCV):\n",
    "    def __init__(self, input_size, hidden_size, lr):\n",
    "        nn.Module.__init__(self)\n",
    "        GridSearchCV.__init__(self, estimator=self, param_grid={})  # Initialize GridSearchCV\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.model = NeuralNetwork(input_size, hidden_size)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Create a DataLoader for training set\n",
    "        train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        # Train the neural network model\n",
    "        for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            return predicted.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "evaluation = []\n",
    "input_size = len(features_columns)\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(quarters):\n",
    "    train_quarters, test_quarters = quarters[train_index], quarters[test_index]\n",
    "    train = labeled_index.loc[train_quarters]\n",
    "    test = labeled_index.loc[test_quarters]\n",
    "    X_train = train[features_columns]\n",
    "    y_train = train['crash_label']\n",
    "    X_test = test[features_columns]\n",
    "    y_test = test['crash_label']\n",
    "\n",
    "    # Oversample the minority class (1) using SMOTE\n",
    "    oversampler = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "    X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Undersample the majority class (0) using RandomUnderSampler\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_oversampled, y_train_oversampled)\n",
    "    \n",
    "    input_size = X_train_resampled.shape[1]  # Number of input features\n",
    "    hidden_size = 64  # Number of neurons in the hidden layer\n",
    "    lr = 0.001  # Learning rate for the neural network\n",
    "    \n",
    "    # Perform grid search to find optimal hyperparameters\n",
    "    # Define the hyperparameter grid for the neural network\n",
    "    param_grid = {'hidden_size': [64, 128, 256],'lr': [0.001, 0.01, 0.1]}\n",
    "    grid_search = GridSearchCV(NeuralNetworkEstimator(input_size, hidden_size, lr), param_grid, verbose=2)    \n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Create a new instance of your neural network model with optimal hyperparameters\n",
    "    model = NeuralNetworkEstimator(input_size, hidden_size, lr)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    decision_values = model.decision_function(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, decision_values)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, decision_values)\n",
    "    \n",
    "    evaluation_result = {\n",
    "        'Train_Start': pd.to_datetime(train_quarters).min(),\n",
    "        'Train_End': pd.to_datetime(train_quarters).max(),\n",
    "        'Test_Start': pd.to_datetime(test_quarters).min(),\n",
    "        'Test_End': pd.to_datetime(test_quarters).max(),\n",
    "        'Confusion_Matrix': conf_matrix,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'AUC-ROC': auc_roc,\n",
    "        'False Positive Rate': fpr,\n",
    "        'True Positive Rate': tpr\n",
    "    }\n",
    "\n",
    "    # Calculate feature importance\n",
    "    feature_importance = np.abs(model.dual_coef_).sum(axis=0)\n",
    "    for name, importance in zip(features_columns, feature_importance):\n",
    "        evaluation_result[f\"{name}_importance\"] = importance\n",
    "\n",
    "    # Append result to evaluation\n",
    "    evaluation.append(evaluation_result)\n",
    "\n",
    "    print(\"Prediction model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d9b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29ff49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
