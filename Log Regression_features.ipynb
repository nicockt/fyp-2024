{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618b70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_percentage_error, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from pylab import rcParams\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pickle\n",
    "import importlib\n",
    "import sys\n",
    "import joblib\n",
    "import warnings\n",
    "import glob\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d26044",
   "metadata": {},
   "source": [
    "## Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eee38cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     volatility      index  crash_label  price_change  volume_change  \\\n",
      "0     -0.628541  000001.SS            0      0.187709      -0.127147   \n",
      "1     -0.628541  000001.SS            0      0.504913      -0.127147   \n",
      "2     -0.625680  000001.SS            1     -0.795446      -0.127147   \n",
      "3     -0.631403  000001.SS            0     -0.843210      -0.127147   \n",
      "4     -0.629515  000001.SS            0     -0.082838      -0.127147   \n",
      "..          ...        ...          ...           ...            ...   \n",
      "875   -0.493234      ^SSMI            0     -0.800378      -0.127147   \n",
      "876   -0.529809      ^SSMI            0      0.914383      -0.127147   \n",
      "877   -0.458321      ^SSMI            0      0.217264      -0.127147   \n",
      "878   -0.388212      ^SSMI            0     -0.010617      -0.127147   \n",
      "879   -0.351813      ^SSMI            0      0.296414      -0.127147   \n",
      "\n",
      "           date  Quarter  Crude_Oil_Index_Excess_Return_Quarterly  \\\n",
      "0    1998-03-31  Q1 1998                                -0.159831   \n",
      "1    1998-06-30  Q2 1998                                -0.200681   \n",
      "2    1998-09-30  Q3 1998                                 0.064836   \n",
      "3    1998-12-31  Q4 1998                                -0.302470   \n",
      "4    1999-03-31  Q1 1999                                 0.366111   \n",
      "..          ...      ...                                      ...   \n",
      "875  2018-12-31  Q4 2018                                -0.383158   \n",
      "876  2019-03-31  Q1 2019                                 0.299060   \n",
      "877  2019-06-30  Q2 2019                                -0.034282   \n",
      "878  2019-09-30  Q3 2019                                -0.074215   \n",
      "879  2019-12-31  Q4 2019                                 0.130346   \n",
      "\n",
      "     TEDRATE_Quarterly  Goldprice_Quarterly  ...  \\\n",
      "0            -0.187008            -0.004248  ...   \n",
      "1             0.178370            -0.012918  ...   \n",
      "2             0.109955            -0.069943  ...   \n",
      "3             0.282704             0.158250  ...   \n",
      "4            -0.417736            -0.048471  ...   \n",
      "..                 ...                  ...  ...   \n",
      "875           0.040517             0.050804  ...   \n",
      "876          -0.050347             0.040094  ...   \n",
      "877          -0.314081             0.039500  ...   \n",
      "878           0.043571             0.113424  ...   \n",
      "879           0.657813            -0.029800  ...   \n",
      "\n",
      "     Long-Term_Government_Bond Yields  current_acct   FX_Rate  turnover  \\\n",
      "0                           -0.009152     -3.126550  0.000000  0.001980   \n",
      "1                           -0.042983     -0.056580  0.000000  0.001980   \n",
      "2                           -0.189173      0.001742  0.000000  0.001980   \n",
      "3                            0.127303     -0.221874  0.000000  0.001980   \n",
      "4                            0.011444     -0.862787  0.000000 -0.000512   \n",
      "..                                ...           ...       ...       ...   \n",
      "875                         -4.000000     -0.574781  0.000000  0.000395   \n",
      "876                          0.285714     -1.342700  0.020202 -0.000479   \n",
      "877                          1.407407      0.681766 -0.019802 -0.000479   \n",
      "878                         -0.215385     -5.799000  0.000000 -0.000479   \n",
      "879                          0.372549      0.123311 -0.020202 -0.000479   \n",
      "\n",
      "     Population       npl  Recession_Indicators  inflation  Unemployment  \\\n",
      "0      0.002391  0.014958                   1.0  -3.126550      0.000000   \n",
      "1      0.002391  0.014958                   1.0  -0.056580      0.000000   \n",
      "2      0.002391  0.014958                   1.0   0.001742      0.000000   \n",
      "3      0.002391  0.014958                   1.0  -0.221874      0.000000   \n",
      "4      0.002168  0.014907                   1.0  -0.862787      0.007710   \n",
      "..          ...       ...                   ...        ...           ...   \n",
      "875    0.001845  0.001651                   1.0  -0.574781     -0.004708   \n",
      "876    0.001795  0.001610                   1.0  -1.342700     -0.017425   \n",
      "877    0.001795  0.074570                   1.0   0.681766     -0.017425   \n",
      "878    0.001795  0.160824                   1.0  -5.799000     -0.017425   \n",
      "879    0.001795  0.247079                   1.0   0.123311     -0.017425   \n",
      "\n",
      "          GDP  \n",
      "0    0.017082  \n",
      "1    0.017082  \n",
      "2    0.017082  \n",
      "3    0.017082  \n",
      "4    0.015415  \n",
      "..        ...  \n",
      "875  0.010750  \n",
      "876 -0.001453  \n",
      "877 -0.001453  \n",
      "878 -0.001453  \n",
      "879 -0.001453  \n",
      "\n",
      "[880 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled index data from csv\n",
    "labeled_index = pd.read_csv(\"./data/labeled_data/quarterly_labeled_index_standardized.csv\")\n",
    "# Concat the index data with features - Q1 1998 - Q4 2019\n",
    "Marco_folder = \"./data/Features_Marco\"\n",
    "Micro_folder = \"./data/updatest\"\n",
    "csv_file_pattern = \"*.csv\"\n",
    "Marco_csv_files = glob.glob(f\"{Marco_folder}/{csv_file_pattern}\")\n",
    "Micro_csv_files = glob.glob(f\"{Micro_folder}/{csv_file_pattern}\")\n",
    "\n",
    "#Marco\n",
    "# Iterate over each CSV file in Marco_csv_files\n",
    "for file in Marco_csv_files:\n",
    "    data = pd.read_csv(file)\n",
    "    # Get the file name from the CSV file path\n",
    "    file_name = file.split('/')[-1]\n",
    "    # Extract the index value from the file name\n",
    "    new_name = file_name.split('.')[0]\n",
    "    # Rename the 'Percentage Change' column to the index value\n",
    "    data.rename(columns={'Percentage Change': new_name}, inplace=True)\n",
    "    \n",
    "    # Merge the data based on the specified conditions\n",
    "    labeled_index = labeled_index.merge(data,\n",
    "                                     how='left',\n",
    "                                     left_on='Quarter',\n",
    "                                     right_on='Quarter')\n",
    "\n",
    "\n",
    "#Micro\n",
    "# Iterate over each CSV file in csv_files\n",
    "for file in Micro_csv_files:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    # Extract the header row and separate the index values\n",
    "    header = data.columns[1:]  # Assuming the index values are in columns except the first one\n",
    "    index_values = header.tolist()\n",
    "    \n",
    "    # Get the file name from the CSV file path\n",
    "    file_name = file.split('/')[-1]\n",
    "    # Extract the index value from the file name\n",
    "    new_name = file_name.split('.')[0]\n",
    "            \n",
    "    # Create lists for the three columns\n",
    "    quarter_column = []\n",
    "    index_column = []\n",
    "    file_name_column = []\n",
    "\n",
    "    # Iterate through the data rows\n",
    "    for row in data.itertuples(index=False):\n",
    "        quarter = row[0]  # Extract the quarter value\n",
    "        values = row[1:]  # Extract the values in the row\n",
    "\n",
    "        # Iterate through the values and extract index and file name\n",
    "        for index, file_name in zip(index_values, values):\n",
    "            # Append the values to their respective columns\n",
    "            quarter_column.append(quarter)\n",
    "            index_column.append(index)\n",
    "            file_name_column.append(file_name)\n",
    "\n",
    "        \n",
    "    # Create a DataFrame from the columns\n",
    "    df = pd.DataFrame({'Quarter': quarter_column, 'index': index_column, new_name: file_name_column})\n",
    "\n",
    "\n",
    "    # Merge the data based on the specified conditions\n",
    "    labeled_index = labeled_index.merge(df,\n",
    "                                        how='left',\n",
    "                                        left_on=['Quarter', 'index'],\n",
    "                                        right_on=['Quarter', 'index'])\n",
    "\n",
    "# Save the labeled_index DataFrame to a CSV file\n",
    "labeled_index.to_csv(\"./data/merge_file.csv\", index=False)\n",
    "\n",
    "# Define quarters\n",
    "quarters = np.sort(labeled_index.index.unique())\n",
    "print(labeled_index)\n",
    "\n",
    "# Save the merge_file DataFrame to a CSV file\n",
    "labeled_index.to_csv(\"./data/merge_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764dc56",
   "metadata": {},
   "source": [
    "## Z-Score Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abcf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = list(labeled_index.columns)\n",
    "columns_to_remove = ['index', 'crash_label', 'date', 'Quarter']\n",
    "features_columns = [column for column in features_columns if column not in columns_to_remove]\n",
    "\n",
    "# Handle extreme value\n",
    "labeled_index['volume_change'].replace([np.inf], 1e10, inplace=True)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected data\n",
    "scaled_features = scaler.fit_transform(labeled_index[features_columns])\n",
    "\n",
    "# Replace the original columns with the scaled values\n",
    "labeled_index[features_columns] = scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f65ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merge_file DataFrame to a CSV file\n",
    "labeled_index.to_csv(\"./data/labeled_data/quarterly_labeled_features_standardized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146fedf",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9712d13c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: Z-score standardization\u001b[39;00m\n\u001b[1;32m      6\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 7\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m tscv\u001b[38;5;241m.\u001b[39msplit(quarters):\n\u001b[1;32m     11\u001b[0m     train_quarters, test_quarters \u001b[38;5;241m=\u001b[39m quarters[train_index], quarters[test_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Call TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "evaluation = []\n",
    "\n",
    "for train_index, test_index in tscv.split(quarters):\n",
    "    \n",
    "    train_quarters, test_quarters = quarters[train_index], quarters[test_index]\n",
    "    train = labeled_index.loc[train_quarters]\n",
    "    test = labeled_index.loc[test_quarters]\n",
    "    X_train = train[features_columns]\n",
    "    y_train = train['crash_label']\n",
    "    X_test = test[features_columns]\n",
    "    y_test = test['crash_label']\n",
    "    \n",
    "    # Oversample the minority class (1) using SMOTE\n",
    "    oversampler = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "    X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Undersample the majority class (0) using RandomUnderSampler\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "    # Hyperparameter Tuning\n",
    "    param_grid = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'solver': ['liblinear', 'sag', 'saga']}\n",
    "    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, verbose=2)\n",
    "    try:\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        print(f\"Best Score: {best_score}\")\n",
    "        print(\"Grid search completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during grid search:\")\n",
    "        print(e)\n",
    "\n",
    "    # Train the model\n",
    "    model = LogisticRegression(C=best_params['C'], solver=best_params['solver'])\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    #joblib.dump(model, 'logistic_regression_model.joblib')\n",
    "    print(\"prediction model trained\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Predict probabilities on the test data\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels= [0,1])\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    \n",
    "    evaluation_result = {\n",
    "        'Train_Start': pd.to_datetime(train_quarters).min(),\n",
    "        'Train_End': pd.to_datetime(train_quarters).max(),\n",
    "        'Test_Start': pd.to_datetime(test_quarters).min(),\n",
    "        'Test_End': pd.to_datetime(test_quarters).max(),\n",
    "        'Confusion_Matrix': conf_matrix,\n",
    "        'Precision': precision, \n",
    "        'Recall': recall, \n",
    "        'F1': f1, \n",
    "        'Accuracy': accuracy, \n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'AUC-ROC': auc_roc,\n",
    "        'False Positive Rate': fpr,\n",
    "        'True Positive Rate': tpr\n",
    "    }\n",
    "    \n",
    "    # Feature Importance\n",
    "    feature_names = list(X_train.columns)\n",
    "    feature_importance = model.coef_[0]\n",
    "    for name, importance in zip(feature_names, feature_importance):\n",
    "        evaluation_result[f\"{name}_importance\"] = importance\n",
    "\n",
    "    # Append result to evaluation\n",
    "    evaluation.append(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame(evaluation)\n",
    "evaluation_df.to_csv('./' + f\"evaluation_benchmark.csv\", index=False)\n",
    "evaluation_df\n",
    "#TODO: visualize it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83d63c",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "- Precision: number of True Positives / (number of True Positives + number of False Positives)\n",
    "- Recall: number of True Positives / (number of True Positives + number of False Negatives)\n",
    "- F1 score: A weighted average of precision and recall, F1 = 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402521ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: python function for plotting Importance, AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train.columns)\n",
    "print(feature_names)\n",
    "# Determine non-numeric attribute names\n",
    "for feature in feature_names:\n",
    "    X_train[feature] = X_train[feature].astype('float64')\n",
    "    \n",
    "feature_importance = model.coef_[0]\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"Feature: {name}, Importance: {importance}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(feature_names, feature_importance)\n",
    "plt.title('Feature Importances in Logistic Regression Model')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction Result\n",
    "y_prob = model.predict_proba(X_test_roc)\n",
    "crash_prob = y_prob[:, 1]\n",
    "print(f\"Crash Probability: {crash_prob.mean()}\")\n",
    "\n",
    "#AUC_ROC\n",
    "auc_roc = roc_auc_score(Y_test_roc, y_prob[:, 1])\n",
    "print(\"AUC-ROC Score:\", auc_roc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test_roc, y_prob[:, -1])\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_roc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
